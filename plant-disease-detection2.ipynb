# First, the simple CNN
!pip install timm

import torch

# Check for GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
print("using device:", device)

from PIL import Image
import matplotlib.pyplot as plt

# Load and display the image
img = Image.open('uploaded_image.jpg')

plt.imshow(img)
plt.axis('off')
plt.show()

# First, manually bring PlantVillage zip folder into session. The next line of code will unzip the folder.
!unzip PlantVillage.zip

import os
from PIL import Image
import matplotlib.pyplot as plt

# Example: Look at one random image
folder = "PlantVillage"  # change if your path is different
class_folder = os.path.join(folder, os.listdir(folder)[0])  # first class
image_file = os.path.join(class_folder, os.listdir(class_folder)[0])

img = Image.open(image_file)

plt.imshow(img)
plt.axis('off')
plt.title("Example Image")
plt.show()

from torchvision import datasets, transforms

# Define image size
IMG_SIZE = 224  # standard size for ViT/Swin

# Define transformations
train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ToTensor(),  # converts to tensor
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # normalize to [-1,1]
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
])

from torch.utils.data import DataLoader, random_split

# Path to the dataset
dataset_path = '/content/PlantVillage'

# Create dataset
full_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)

# Split into train/test
train_size = int(0.8 * len(full_dataset))
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

# Create data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

import torch
import torch.nn as nn
import torch.nn.functional as F

# Define a simple CNN
class SimpleCNN(nn.Module):
    def __init__(self, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(32 * (IMG_SIZE // 4) * (IMG_SIZE // 4), 128)
        self.fc2 = nn.Linear(128, num_classes)
        self.dropout = nn.Dropout(0.3)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)  # flatten
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

num_classes = len(full_dataset.classes)
print("Number of classes:", num_classes) # There should be 38 classes!

model = SimpleCNN(num_classes=num_classes).to(device)

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Number of epochs
epochs = 5

for epoch in range(epochs):
    running_loss = 0.0
    model.train()

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()  # zero the parameter gradients

        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}")

print("Finished Training!")

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))

    print("Confusion Matrix:")
    cm = confusion_matrix(all_labels, all_preds)
    print(cm)

evaluate_model(model, test_loader)

# Next, the fine-tuned pretrained ViT

!pip install transformers timm

from transformers import ViTForImageClassification, ViTImageProcessor

# Load the image processor
image_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')

# Load the pretrained ViT model and adapt it for 38 classes
model = ViTForImageClassification.from_pretrained(
    'google/vit-base-patch16-224',
    num_labels=num_classes,
    ignore_mismatched_sizes=True  # This lets it replace the classification head
).to(device)

from torchvision import transforms

IMG_SIZE = 224  # for ViT

train_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
])

test_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)
])

# Reload dataset with new ViT transforms
full_dataset = datasets.ImageFolder(root=dataset_path, transform=train_transform)

# Re-split
train_size = int(0.8 * len(full_dataset))
test_size = len(full_dataset) - train_size
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

# New dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

from torch.optim import AdamW

criterion = nn.CrossEntropyLoss()
optimizer = AdamW(model.parameters(), lr=2e-5)  # smaller learning rate for ViT

epochs = 5

for epoch in range(epochs):
    running_loss = 0.0
    model.train()

    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()

        outputs = model(inputs).logits  # <-- important! use .logits
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}")

print("Finished Fine-tuning Vision Transformer!")

torch.save(model.state_dict(), 'vit-plantvillage.pth')

# This is for saving the model to Google Drive permanently
from google.colab import drive
drive.mount('/content/drive')

!cp vit-plantvillage.pth /content/drive/MyDrive/

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            logits = outputs.logits  # <--- Get the logits tensor from the Hugging Face output
            _, preds = torch.max(logits, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print("Classification Report:")
    print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))

    print("Confusion Matrix:")
    cm = confusion_matrix(all_labels, all_preds)
    print(cm)

evaluate_model(model, test_loader)

print(full_dataset.classes) # Full list of class names must be displayed in order on the app.py file. This will print them.


